# -*- coding: utf-8 -*-
"""Copy of orginal of loan_approval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rA7JvSz3vovHZ28HIOxjqZFXIqgEutJN

###INTRO
The Loan Approval Prediction project aims to build machine learning models that can accurately predict whether a loan application should be approved or not, based on various applicant features such as income, credit history, employment status,loan amount  and property status. Automating the loan approval process using data-driven techniques can help financial institutions make faster and more consistent decisions, reduce manual errors, and improve overall customer experience.

Importing libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("/content/loan_approval.csv") #load csv dataset into pandas dataframe
df

"""preprocessing"""

df.head()

df.tail()

df.info()

df.describe()

"""checking null"""

df.isna()

df.isna().sum()

df.dropna(inplace=True) #drop null values
df

df.duplicated().sum()

"""Boxploting for checking outliers"""

plt.boxplot(df['person_income'])

plt.boxplot(df['person_emp_exp'])

plt.boxplot(df['loan_amnt'])

plt.boxplot(df['cb_person_cred_hist_length'])

"""Handling outliers"""

df['person_income'] = pd.to_numeric(df['person_income'])
Q1=df['person_income'].quantile(0.25)
Q3=df['person_income'].quantile(0.75)
IQR=Q3-Q1

lowerbound=Q1-(IQR*1.5)
upperbound=Q3+(IQR*1.5)

df=df[~
    ((df['person_income']<(lowerbound))|(
        df['person_income']>(upperbound)))]

import matplotlib.pyplot as plt
plt.boxplot(df['person_income'])
plt.show()

df['person_emp_exp'] = pd.to_numeric(df['person_emp_exp'])
Q1=df['person_emp_exp'].quantile(0.25)
Q3=df['person_emp_exp'].quantile(0.75)
IQR=Q3-Q1

lowerbound=Q1-(IQR*1.5)
upperbound=Q3+(IQR*1.5)

df=df[~
    ((df['person_emp_exp']<(lowerbound))|(
        df['person_emp_exp']>(upperbound)))]

import matplotlib.pyplot as plt
plt.boxplot(df['person_emp_exp'])
plt.show()

df['loan_amnt'] = pd.to_numeric(df['loan_amnt'])
Q1=df['loan_amnt'].quantile(0.25)
Q3=df['loan_amnt'].quantile(0.75)
IQR=Q3-Q1

lowerbound=Q1-(IQR*1.5)
upperbound=Q3+(IQR*1.5)

df=df[~
    ((df['loan_amnt']<(lowerbound))|(
        df['loan_amnt']>(upperbound)))]

import matplotlib.pyplot as plt
plt.boxplot(df['loan_amnt'])
plt.show()

df['cb_person_cred_hist_length'] = pd.to_numeric(df['cb_person_cred_hist_length'])
Q1=df['cb_person_cred_hist_length'].quantile(0.25)
Q3=df['cb_person_cred_hist_length'].quantile(0.75)
IQR=Q3-Q1

lowerbound=Q1-(IQR*1.5)
upperbound=Q3+(IQR*1.5)

df=df[~
    ((df['cb_person_cred_hist_length']<(lowerbound))|(
        df['cb_person_cred_hist_length']>(upperbound)))]

import matplotlib.pyplot as plt
plt.boxplot(df['cb_person_cred_hist_length'])
plt.show()

"""label encoding"""

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['person_gender'] = le.fit_transform(df['person_gender'])
df

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['person_education'] = le.fit_transform(df['person_education'])
df

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['person_home_ownership'] = le.fit_transform(df['person_home_ownership'])
df

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['loan_intent'] = le.fit_transform(df['loan_intent'])
df

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['previous_loan_defaults_on_file'] = le.fit_transform(df['previous_loan_defaults_on_file'])
df

df.corr() #correlation of columns

corr_matrix = df.corr(numeric_only=True)  #compute corelation only for numeric colowmns

plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=".2f", square=True)  #plot matrix with color coding
plt.title('Correlation Matrix of Numerical Features')
plt.tight_layout()
plt.show()

df[['person_age', 'person_income', 'person_emp_exp','loan_amnt','loan_int_rate','credit_score']].hist(bins=8, figsize=(12, 4), color='skyblue', edgecolor='black')
#plots histogram for several colwmns,showing the distribuition of each

from sklearn.preprocessing import MinMaxScaler   #min max normalization
#scale features
def normalize_column(df, person_income):
    scaler = MinMaxScaler()
    df[person_income] = scaler.fit_transform(df[[person_income]])
    return df

df = normalize_column(df, 'person_income')
df

from sklearn.preprocessing import StandardScaler

#Standardization:stdandardize feature
standard_scaler = StandardScaler()
df_standard_scaled = pd.DataFrame(standard_scaler.fit_transform(df), columns=df.columns)
print("\nStandard Scaled Data :\n", df_standard_scaled.head())

"""Train split"""

from sklearn.model_selection import train_test_split
X= df.drop('loan_status', axis=1)
y = df['loan_status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(df.shape)
print(X.shape)
print(y.shape)
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

"""#logistic Regression"""

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(X_train, y_train)

y_test  #containing the true labels for the test set

y_pred = lr.predict(X_test)    #predict the loan status
y_pred

from sklearn.metrics import accuracy_score   #Accuracy score
print(accuracy_score(y_test, y_pred))

from sklearn.metrics import confusion_matrix, classification_report  #shows matrix comparing pred vs actual
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

#plot graph showing confusion matrix
import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""#KNN"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""##Hyperparameter tunning KNN K=7"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)           #value of k(hyperparameter n_neighbors)=7
knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""#random forest:  categorical outcomes are predicted"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=50)
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)

print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""##hyperparameter tunning RF"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=150)          # value of n_estimators=150
rf.fit(X_train, y_train)

y_pred = rf.predict(X_test)

print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""#SVM"""

from sklearn.svm import SVC
svc = SVC(kernel='linear')                 # kernel=linear
svc.fit(X_train, y_train)

y_pred = svc.predict(X_test)
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""##Hyperparameter tunning SVM"""

from sklearn.svm import SVC
svc = SVC(kernel='rbf')                 # kernel=rbf
svc.fit(X_train, y_train)

y_pred = svc.predict(X_test)
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""#GNB"""

from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()
nb.fit(X_train, y_train)

y_pred = nb.predict(X_test)
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""#Decision tree classifier"""

from sklearn.tree import DecisionTreeClassifier  #predicting categorical outcomes
dtc = DecisionTreeClassifier()
dtc.fit(X_train, y_train)

y_pred = dtc.predict(X_test)

print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""#MLP classifiers"""

from sklearn.neural_network import MLPClassifier
#Create an MLP classifier
mlp = MLPClassifier(hidden_layer_sizes=(10,10,5), max_iter=1000, random_state=42)
# Train the MLP model
mlp.fit(X_train, y_train)
# Make prediction
y_pred = mlp.predict(X_test)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""#gradient boosting  classifier   """

import pandas as pd
import sklearn.metrics
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,accuracy_score
from sklearn.datasets import load_digits

# Instantiate the classifier
gb_classifier = GradientBoostingClassifier(
    n_estimators=300,      # Number of trees
    learning_rate=0.05,    # Contribution of each tree
    max_features=5,        # Number of features considered for splits
    random_state=42
)

# Train the model
gb_classifier.fit(X_train, y_train)

# Predict on test data
y_pred = gb_classifier.predict(X_test)

# Classification report and accuracy
print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""#XGB classifier"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

#split into training and training sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f"training samples:{X_train.shape[0]},testing samples:{X_test.shape[0]}")  #sample size

#create an XGBoost classifier instance
model=XGBClassifier(
    n_estimators=100,      #no of boosting rounds
    learning_rate=0.1,      #max tree depth
    max_depth=3,            #stepsize shrinkagr
    objective='binary:logistic',   #binary classification
    random_state=42
)

#train the model
model.fit(X_train, y_train)

y_pred=model.predict(X_test)

print(confusion_matrix(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))

#hyperparameter tuning using GridSearchCV with RandomForestClassifier
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5]
}

grid = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid.fit(X_train, y_train)

print("Best parameters:", grid.best_params_)
print("Best score:", grid.best_score_)

"""#Compare the classifiers
| Model                | Hyperparameter(s)    | Accuracy (%) | Precision | Recall | F1-Score |
|----------------------|---------------------|--------------|-----------|--------|----------|
| Logistic Regression   | —                   | 85%        | 0.84      | 0.85   | 0.84     |
| kNN                  | k=5                 | 76%         | 0.71      | 0.76   | 0.72     |
| KNN                  | k=7                 | 77%        | 0.72      | 0.77  | 0.72    |
| random forest| n=50|92%|0.92|0.92|0.92|
|random forest|n=150|92%|0.92|0.92|0.92
| SVM                  | kernel='linear'     | 87%         | 0.86      | 0.87   | 0.86     |
| SVM                  | kernel='rbf'        | 78%        | 0.61      | 0.78 | 0.69    |
| Gaussian naive bayes                  |        | 84%         | 0.88      | 0.84   | 0.85     |
| Decision tree c                  |         | 90%         | 0.90      | 0.90   | 0.90     |
| MLP                  |     | 83%         | 0.82      | 0.83   | 0.79     |
| GBC                  |        | 92%         | 0.92    | 0.92  | 0.92     |
|XGB|   | 92%|0.92|0.92|0.92|

#Conclusion
**Random Forest (n=50 & n=150), Gradient Boosting (GBC), and XGBoost (XGB)**  consistently achieved the highest accuracy of 92% along with excellent precision, recall, and F1-scores (0.92), indicating balanced and robust performance.

**Decision Tree Classifier** also performed strongly with 90% accuracy, showing that even a single-tree model can be competitive.

**Logistic Regression and SVM (linear kernel)** delivered decent performance (85–87% accuracy), suitable for simpler or linearly separable problems.

**k-Nearest Neighbors (k=5 and k=7) and SVM (rbf kernel)** underperformed, suggesting these models may not generalize well on this dataset or require further tuning.

**Gaussian Naive Bayes and MLP** showed moderate performance, but were outperformed by ensemble models.                                                                          


Random Forest, Gradient Boosting, and XGBoost are recommended due to their high accuracy, strong generalization, and balanced metrics. These models are well-suited for production deployment or further optimization.
"""